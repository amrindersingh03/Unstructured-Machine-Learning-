{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGGDUbAMwG1H0wKGtcv9iI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrindersingh03/Unstructured-Machine-Learning-/blob/main/Langchain_bot_zenml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our aim is to construct a Langchain bot that will retrieve every markdown file from ZenML's repository and use them to answer questions and gather relevant information.\n",
        "\n"
      ],
      "metadata": {
        "id": "nX72Ygy9lQYu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpQ7cbxRe-cY",
        "outputId": "bc198827-c1a1-465a-b436-1311d1b584ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain==0.0.55\n",
            "  Downloading langchain-0.0.55-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.27.1)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.55) (1.10.6)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.55) (1.22.4)\n",
            "Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.55) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.55) (1.4.46)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (3.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.55) (2.0.2)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, faiss-cpu, multidict, frozenlist, async-timeout, yarl, langchain, huggingface-hub, aiosignal, transformers, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 faiss-cpu-1.7.3 frozenlist-1.3.3 huggingface-hub-0.13.2 langchain-0.0.55 multidict-6.0.4 openai-0.27.2 tokenizers-0.13.2 transformers-4.27.1 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "# Install Langchain\n",
        "\n",
        "!pip install langchain==0.0.55 requests openai transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import OpenAI library. We will use various tools from open ai library to create our chat bot"
      ],
      "metadata": {
        "id": "2vKbu_2tl4Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n"
      ],
      "metadata": {
        "id": "AGuMzv0zfGN4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To access the OpenAI environment, you can import it using your unique API key provided by OpenAI."
      ],
      "metadata": {
        "id": "vHFwo8ORncDW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...Enter OpenAI API Key here...\""
      ],
      "metadata": {
        "id": "b43oy47HfRjw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's fetch all markdown files of ZenML's repository"
      ],
      "metadata": {
        "id": "_pSvir8qoaFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required tools to create the function to get ZenML's github repository.\n",
        "\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "from langchain.docstore.document import Document\n",
        "import requests"
      ],
      "metadata": {
        "id": "JeZGOLFHvHq7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to get markdown files."
      ],
      "metadata": {
        "id": "j1H0Y_Dco5ub"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_github_docs(repo_owner, repo_name):\n",
        "    with tempfile.TemporaryDirectory() as d:\n",
        "        subprocess.check_call(\n",
        "            f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\n",
        "            cwd=d,\n",
        "            shell=True,\n",
        "        )\n",
        "        git_sha = (\n",
        "            subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\n",
        "            .decode(\"utf-8\")\n",
        "            .strip()\n",
        "        )\n",
        "        repo_path = pathlib.Path(d)\n",
        "        markdown_files = list(repo_path.glob(\"*/*.md\")) + list(\n",
        "            repo_path.glob(\"*/*.mdx\")\n",
        "        )\n",
        "        for markdown_file in markdown_files:\n",
        "            with open(markdown_file, \"r\") as f:\n",
        "                relative_path = markdown_file.relative_to(repo_path)\n",
        "                github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n",
        "                yield Document(page_content=f.read(), metadata={\"source\": github_url})"
      ],
      "metadata": {
        "id": "2X-JW-EtvUnU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following actions are performed by this process:\n",
        "\n",
        "- It obtains the latest commit of the intended GitHub repository and saves it in a temporary directory.\n",
        "\n",
        "- It retrieves the git sha, which is used to generate links that the model can reference in its sources list.\n",
        "\n",
        "- It scans through each markdown file (.md or .mdx) in the repository.\n",
        "\n",
        "- It generates a URL to the markdown file on GitHub, reads the file from the local storage, and produces a Document object."
      ],
      "metadata": {
        "id": "IR-a1PNfqhO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use our above function to  fetch md or mdx file data from ZenML repo and store in a varaiable named \" sources \""
      ],
      "metadata": {
        "id": "-c93ZSPtqpKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sources = get_github_docs(\"zenml-io\", \"zenml\")"
      ],
      "metadata": {
        "id": "LZJ6uJIsvY8T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Dividing lengthy text into smaller segments can be crucial to handle it efficiently.\n",
        "\n",
        "To accomplish this, we can use the CharacterTextSplitter technique, which separates the text into segments and stores them in a list."
      ],
      "metadata": {
        "id": "erBL6ywiron0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Let's import CharacterTextSplitter and break down our documents into small chunks. \n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "3YdLv9780NoS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_chunks = []\n",
        "splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "for source in sources:\n",
        "    for chunk in splitter.split_text(source.page_content):\n",
        "        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))"
      ],
      "metadata": {
        "id": "pXtE9fF70Hcp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now, we need to Import the FAISS library, which is a similarity search tool developed by Facebook AI. Its primary function is to allow for rapid searching of embeddings in multimedia documents that are similar to one another. In other words, it will search for text in transcriptions that closely match the text in our question or search.\n",
        "\n",
        "- To use FAISS, the input data must be in the form of embeddings, or vectors. Therefore, before feeding data to FAISS, the text must first be converted into embeddings.\n",
        "\n",
        "- Once the text has been converted into embeddings, FAISS can compare and search for similar embeddings to those found in the question.\n",
        "\n",
        "- To create embeddings, we will use a tool by OpenAI called OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "hPqMlOGhunva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "7y6z51hgtO8B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's transform our text documents into vector form using FAISS"
      ],
      "metadata": {
        "id": "EJNN4rqwtN8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "YLGI7_uUvd1J"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import load_qa_with_sources_chain tool from LangChainAI library."
      ],
      "metadata": {
        "id": "4mNWHCuh5s6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
      ],
      "metadata": {
        "id": "tCKg6bGd59r-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a chain using load_qa_with_sources_chain tool\n",
        "\n",
        "load_qa_with_sources_chain will take in the query  and lookup for the documents from the vector database (created by FAISS) of markdown files data, stored in variable 'search_index'.\n",
        "\n"
      ],
      "metadata": {
        "id": "meF7D1ye6hwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_with_sources_chain(OpenAI(temperature=0))\n",
        "\n",
        "def print_answer(question):\n",
        "    print(\n",
        "        chain(\n",
        "            {\n",
        "                \"input_documents\": search_index.similarity_search(question, k=4),\n",
        "                \"question\": question,\n",
        "            },\n",
        "            return_only_outputs=True,\n",
        "        )[\"output_text\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "DqHult273m64"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright! Time to ask some questions to the bot."
      ],
      "metadata": {
        "id": "4xBVsq3X7guw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"what is zenml framework?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKZ_wIz73vpF",
        "outputId": "a427d71b-fe4d-4a7f-9689-828c16a335c3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ZenML is an open-source machine learning framework that enables data scientists and engineers to quickly develop, manage, and deploy ML pipelines. It provides a unified platform for data scientists and engineers to develop, manage, and deploy ML pipelines.\n",
            "\n",
            "SOURCES: https://github.com/zenml-io/zenml/blob/52a777f347282987fbaedf177fb6a2342ae9a7b7/tests/README.md, https://github.com/zenml-io/zenml/blob/52a777f347282987fbaedf177fb6a2342ae9a7b7/examples/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"what are the components of a ZenML Deployment?\")"
      ],
      "metadata": {
        "id": "NSgW3U2a8DRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faffdefe-72b4-4d54-ddfb-68d9ff73ecbe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The components of a ZenML Deployment include a ZenML server running locally as a daemon process, a ZenML server running in a Docker container, a ZenML server and a MySQL server both running in Docker containers and managed by Docker Compose, and an external ZenML server running in the cloud.\n",
            "SOURCES: https://github.com/zenml-io/zenml/blob/52a777f347282987fbaedf177fb6a2342ae9a7b7/tests/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"How to construct the API docs locally?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMhBXWXmeZnD",
        "outputId": "d4b83686-f118-44fe-c718-4e11c35e4988"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " To construct the API docs locally, clone the repository, install ZenML and all dependencies, and run `bash scripts/serve_api_docs.sh` from the repository root.\n",
            "SOURCES: https://github.com/zenml-io/zenml/blob/52a777f347282987fbaedf177fb6a2342ae9a7b7/docs/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! Our bot has correctly answered all of our questions, and also is able to provide us the source of information.\n"
      ],
      "metadata": {
        "id": "O0XF3sDY8E8O"
      }
    }
  ]
}