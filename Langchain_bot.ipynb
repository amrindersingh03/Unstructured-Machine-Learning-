{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnravGvPUG6YwVocE1uDh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrindersingh03/Unstructured-Machine-Learning-/blob/main/Langchain_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our aim is to construct a language chain bot that will retrieve every markdown file from Cohere's repository and use them to generate questions and gather relevant answers and information."
      ],
      "metadata": {
        "id": "nX72Ygy9lQYu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpQ7cbxRe-cY",
        "outputId": "9a466576-ab95-4992-e675-b71f049b8060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain==0.0.55\n",
            "  Downloading langchain-0.0.55-py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.25.1)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.27.0-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.55) (6.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.55) (1.22.4)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.55) (1.4.46)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain==0.0.55) (1.10.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (2.10)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.55) (2.0.2)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: tokenizers, faiss-cpu, multidict, frozenlist, charset-normalizer, async-timeout, yarl, langchain, huggingface-hub, aiosignal, transformers, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 charset-normalizer-3.1.0 faiss-cpu-1.7.3 frozenlist-1.3.3 huggingface-hub-0.13.2 langchain-0.0.55 multidict-6.0.4 openai-0.27.2 tokenizers-0.13.2 transformers-4.27.0 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "# Install Langchain\n",
        "\n",
        "pip install langchain==0.0.55 requests openai transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import OpenAI library. We will use various tools from open ai library to create our chat bot"
      ],
      "metadata": {
        "id": "2vKbu_2tl4Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "AGuMzv0zfGN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To access the OpenAI environment, you can import it using your unique API key provided by OpenAI."
      ],
      "metadata": {
        "id": "vHFwo8ORncDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "b43oy47HfRjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's fetch all markdown files of Cohere's repository"
      ],
      "metadata": {
        "id": "_pSvir8qoaFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required tools to fetch Cohere's github repository.\n",
        "\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "from langchain.docstore.document import Document\n",
        "import requests"
      ],
      "metadata": {
        "id": "JeZGOLFHvHq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function to get markdown files."
      ],
      "metadata": {
        "id": "j1H0Y_Dco5ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_github_docs(repo_owner, repo_name):\n",
        "    with tempfile.TemporaryDirectory() as d:\n",
        "        subprocess.check_call(\n",
        "            f\"git clone --depth 1 https://github.com/{repo_owner}/{repo_name}.git .\",\n",
        "            cwd=d,\n",
        "            shell=True,\n",
        "        )\n",
        "        git_sha = (\n",
        "            subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\n",
        "            .decode(\"utf-8\")\n",
        "            .strip()\n",
        "        )\n",
        "        repo_path = pathlib.Path(d)\n",
        "        markdown_files = list(repo_path.glob(\"*/*.md\")) + list(\n",
        "            repo_path.glob(\"*/*.mdx\")\n",
        "        )\n",
        "        for markdown_file in markdown_files:\n",
        "            with open(markdown_file, \"r\") as f:\n",
        "                relative_path = markdown_file.relative_to(repo_path)\n",
        "                github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n",
        "                yield Document(page_content=f.read(), metadata={\"source\": github_url})"
      ],
      "metadata": {
        "id": "2X-JW-EtvUnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following actions are performed by this process:\n",
        "\n",
        "It obtains the latest commit of the intended GitHub repository and saves it in a temporary directory.\n",
        "\n",
        "It retrieves the git sha, which is used to generate links that the model can reference in its sources list.\n",
        "\n",
        "It scans through each markdown file (.md or .mdx) in the repository.\n",
        "\n",
        "It generates a URL to the markdown file on GitHub, reads the file from the local storage, and produces a Document object."
      ],
      "metadata": {
        "id": "IR-a1PNfqhO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will store our fetched data in a varaiable named \" sources \""
      ],
      "metadata": {
        "id": "-c93ZSPtqpKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sources = get_github_docs(\"cohere-ai\", \"tokenizers\")"
      ],
      "metadata": {
        "id": "LZJ6uJIsvY8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing lengthy text into smaller segments can be crucial to handle it efficiently.\n",
        "\n",
        "# To accomplish this, we can use the CharacterTextSplitter technique, which separates the text into segments and stores them in a list"
      ],
      "metadata": {
        "id": "q6WGkpFzredJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Dividing lengthy text into smaller segments can be crucial to handle it efficiently.\n",
        "\n",
        "To accomplish this, we can use the CharacterTextSplitter technique, which separates the text into segments and stores them in a list"
      ],
      "metadata": {
        "id": "erBL6ywiron0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Let's import CharacterTextSplitter and break down our documents into small chunks. \n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "3YdLv9780NoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_chunks = []\n",
        "splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "for source in sources:\n",
        "    for chunk in splitter.split_text(source.page_content):\n",
        "        source_chunks.append(Document(page_content=chunk, metadata=source.metadata))"
      ],
      "metadata": {
        "id": "pXtE9fF70Hcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQR-zD5Dum9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to install the FAISS library, which is a similarity search tool developed by Facebook AI. Its primary function is to allow for rapid searching of embeddings in multimedia documents that are similar to one another. In other words, it will search for text in transcriptions that closely match the text in our question or search.\n",
        "\n",
        "To use FAISS, the input data must be in the form of embeddings, or vectors. Therefore, before feeding data to FAISS, the text must first be converted into embeddings.\n",
        "\n",
        "Once the text has been converted into embeddings, FAISS can compare and search for similar embeddings to those found in the question.\n",
        "\n",
        "To create embeddings, we will use Oa tool called OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "hPqMlOGhunva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "7y6z51hgtO8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's transform our text documents into vector form using FAISS"
      ],
      "metadata": {
        "id": "EJNN4rqwtN8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "YLGI7_uUvd1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import load_qa_with_sources_chain tool from @LangChainAI library."
      ],
      "metadata": {
        "id": "4mNWHCuh5s6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
      ],
      "metadata": {
        "id": "tCKg6bGd59r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a chain using load_qa_with_sources_chain tool\n",
        "\n",
        "load_qa_with_sources_chain will take in the query  and lookup for the documents from the vector database (created by FAISS) of markdown files data, stored in variable 'storesearch_index'.\n",
        "\n"
      ],
      "metadata": {
        "id": "meF7D1ye6hwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_with_sources_chain(OpenAI(temperature=0))\n",
        "\n",
        "def print_answer(question):\n",
        "    print(\n",
        "        chain(\n",
        "            {\n",
        "                \"input_documents\": search_index.similarity_search(question, k=4),\n",
        "                \"question\": question,\n",
        "            },\n",
        "            return_only_outputs=True,\n",
        "        )[\"output_text\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "DqHult273m64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright! Time to ask some questions to the bot"
      ],
      "metadata": {
        "id": "4xBVsq3X7guw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"what is a software defined asset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKZ_wIz73vpF",
        "outputId": "b4f94def-b27e-4a13-bc35-74189c078739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A software defined asset is a piece of software that can be managed and configured using software.\n",
            "SOURCES: https://github.com/dagster-io/dagster/blob/1985cdb22d26f4150b86bd7643c1fc9a21ea59e6/docs/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sources = get_github_docs(\"cohere-ai\", \"tokenizers\")"
      ],
      "metadata": {
        "id": "9i5bOym76dNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"What is cohere's tokenizers library\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWz20W7j6gQj",
        "outputId": "781008d5-9da4-44b6-bc93-63d3cea438a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cohere's tokenizers library is a core written in Rust that provides an implementation of today's most used tokenizers, with a focus on performance and versatility. It works as a pipeline, processing raw text as input and outputs an Encoding.\n",
            "SOURCES: https://github.com/cohere-ai/tokenizers/blob/1dc19e0dd4ba73af938ff4dc80b4b1cd40edf4bf/tokenizers/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NSgW3U2a8DRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! Our bot has answered our question, and also is able to provide us the source of information.\n",
        "\n",
        "Now the problem is , our bot is little slow and expensive. Reason is every time we ask a question, it has to execute everything from scratch. \n",
        "\n",
        "As we know, Open AI charges per token to process. So everytime our bot process it is charged\n",
        "\n",
        "Solution is to cache our data"
      ],
      "metadata": {
        "id": "O0XF3sDY8E8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dagster dagit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTi5Yux2yC6v",
        "outputId": "4a2aedbb-8a3a-40b7-bfb3-4d3ffd91865c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dagster in /usr/local/lib/python3.9/dist-packages (1.2.1)\n",
            "Requirement already satisfied: dagit in /usr/local/lib/python3.9/dist-packages (1.2.1)\n",
            "Requirement already satisfied: universal-pathlib in /usr/local/lib/python3.9/dist-packages (from dagster) (0.0.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from dagster) (2022.7.1)\n",
            "Requirement already satisfied: grpcio-health-checking>=1.44.0 in /usr/local/lib/python3.9/dist-packages (from dagster) (1.47.5)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.9/dist-packages (from dagster) (2.0.1)\n",
            "Requirement already satisfied: grpcio<1.48.1,>=1.44.0 in /usr/local/lib/python3.9/dist-packages (from dagster) (1.47.5)\n",
            "Requirement already satisfied: click>=5.0 in /usr/local/lib/python3.9/dist-packages (from dagster) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from dagster) (4.65.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.9/dist-packages (from dagster) (3.1.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from dagster) (1.10.6)\n",
            "Requirement already satisfied: coloredlogs<=14.0,>=6.1 in /usr/local/lib/python3.9/dist-packages (from dagster) (14.0)\n",
            "Requirement already satisfied: sqlalchemy<2.0.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from dagster) (1.4.46)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.9/dist-packages (from dagster) (4.22.1)\n",
            "Requirement already satisfied: croniter>=0.3.34 in /usr/local/lib/python3.9/dist-packages (from dagster) (1.3.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from dagster) (63.4.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from dagster) (0.8.10)\n",
            "Requirement already satisfied: docstring-parser in /usr/local/lib/python3.9/dist-packages (from dagster) (0.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from dagster) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/dist-packages (from dagster) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from dagster) (2.28.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.9/dist-packages (from dagster) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from dagster) (23.0)\n",
            "Requirement already satisfied: toposort>=1.0 in /usr/local/lib/python3.9/dist-packages (from dagster) (1.10)\n",
            "Requirement already satisfied: pendulum in /usr/local/lib/python3.9/dist-packages (from dagster) (2.1.2)\n",
            "Requirement already satisfied: alembic!=1.6.3,!=1.7.0,>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from dagster) (1.10.2)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from dagster) (2.3.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (from dagster) (1.0.0)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.9/dist-packages (from dagit) (0.21.0)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.9/dist-packages (from dagit) (0.26.1)\n",
            "Requirement already satisfied: dagster-graphql==1.2.1 in /usr/local/lib/python3.9/dist-packages (from dagit) (1.2.1)\n",
            "Requirement already satisfied: graphene>=3 in /usr/local/lib/python3.9/dist-packages (from dagster-graphql==1.2.1->dagit) (3.2.2)\n",
            "Requirement already satisfied: gql[requests]>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from dagster-graphql==1.2.1->dagit) (3.4.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.9/dist-packages (from alembic!=1.6.3,!=1.7.0,>=1.2.1->dagster) (1.2.4)\n",
            "Requirement already satisfied: humanfriendly>=7.1 in /usr/local/lib/python3.9/dist-packages (from coloredlogs<=14.0,>=6.1->dagster) (10.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.9/dist-packages (from grpcio<1.48.1,>=1.44.0->dagster) (1.15.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy<2.0.0,>=1.0->dagster) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2->dagster) (2.1.2)\n",
            "Requirement already satisfied: pytzdata>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pendulum->dagster) (2020.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->dagster) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->dagster) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->dagster) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->dagster) (2022.12.7)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from starlette->dagit) (3.6.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from universal-pathlib->dagster) (2023.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]->dagit) (0.14.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]->dagit) (10.4)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]->dagit) (0.18.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]->dagit) (0.17.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn[standard]->dagit) (0.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<5,>=3.4.0->starlette->dagit) (1.3.0)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.9/dist-packages (from gql[requests]>=3.0.0->dagster-graphql==1.2.1->dagit) (2.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.9/dist-packages (from gql[requests]>=3.0.0->dagster-graphql==1.2.1->dagit) (1.8.2)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.2 in /usr/local/lib/python3.9/dist-packages (from gql[requests]>=3.0.0->dagster-graphql==1.2.1->dagit) (3.2.3)\n",
            "Requirement already satisfied: requests-toolbelt<1,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from gql[requests]>=3.0.0->dagster-graphql==1.2.1->dagit) (0.10.1)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.9/dist-packages (from graphene>=3->dagster-graphql==1.2.1->dagit) (3.2.0)\n",
            "Requirement already satisfied: aniso8601<10,>=8 in /usr/local/lib/python3.9/dist-packages (from graphene>=3->dagster-graphql==1.2.1->dagit) (9.0.1)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.9/dist-packages (from yarl<2.0,>=1.6->gql[requests]>=3.0.0->dagster-graphql==1.2.1->dagit) (6.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dagster import asset\n",
        "import pickle"
      ],
      "metadata": {
        "id": "t7GeTAalzKx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dagster import asset\n",
        "\n",
        "@asset\n",
        "def source_docs():\n",
        "    return list(get_github_docs(\"dagster-io\", \"dagster\"))"
      ],
      "metadata": {
        "id": "Xuk_DSPXzbD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = source_docs()"
      ],
      "metadata": {
        "id": "mEu9VJBRVhQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@asset\n",
        "def search_index(source_docs):\n",
        "    source_chunks = []\n",
        "    splitter = CharacterTextSplitter(separator=\" \", chunk_size=1024, chunk_overlap=0)\n",
        "    for source in source_docs:\n",
        "        for chunk in splitter.split_text(source.page_content):\n",
        "            source_chunks.append(Document(page_content=chunk, metadata=source.metadata))\n",
        "\n",
        "    with open(\"search_index.pickle\", \"wb\") as f:\n",
        "        pickle.dump(FAISS.from_documents(source_chunks, OpenAIEmbeddings()), f)"
      ],
      "metadata": {
        "id": "xbqLTSQnLIT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = search_index(x)"
      ],
      "metadata": {
        "id": "5MS_hJAFVqFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_answer(question):\n",
        "    with open(\"search_index.pickle\", \"rb\") as f:\n",
        "        search_index = pickle.load(f)\n",
        "    print(\n",
        "        chain(\n",
        "            {\n",
        "                \"input_documents\": search_index.similarity_search(question, k=4),\n",
        "                \"question\": question,\n",
        "            },\n",
        "            return_only_outputs=True,\n",
        "        )[\"output_text\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "tz8iUmF7LI4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "yC7SeZe6LYEw",
        "outputId": "8113324c-7aa3-4e89-a2d3-48ae431ccb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-5dd5d5f7b021>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dagit -f Untitled2.ipynb\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"What is cohere's tokenizers library\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7No37xSkLeeQ",
        "outputId": "86d2e9aa-fb49-42d6-e4c5-de6b084f7209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cohere's tokenizers library is a library for tokenizing text.\n",
            "SOURCES: https://github.com/dagster-io/dagster/blob/866577f925c09a7bdc4dadedf285dd41abb8f006/docs/README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answer(\"what is a software defined asset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_XA9ZrrL06X",
        "outputId": "7e971a1e-93ff-4a6c-d63c-2acd7c844b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A software defined asset is a piece of software that can be used to manage and control other software.\n",
            "SOURCES: https://github.com/dagster-io/dagster/blob/866577f925c09a7bdc4dadedf285dd41abb8f006/docs/README.md\n"
          ]
        }
      ]
    }
  ]
}